{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import SparkConf, SparkContext\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '6g')\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, col, explode, concat_ws, collect_list, sort_array, count, size, coalesce, expr, regexp_replace\n",
    "spark= SparkSession.builder.config('spark.ui.port','4040').getOrCreate()\n",
    "\n",
    "spark.conf.set('spark.sql.caseSensitive', 'True') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WoS = spark.read.format(\"parquet\").load(\"/WoSraw_2020_all/parquet/part*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WoS = WoS.filter(WoS.UID.contains('WOS:000294118200001'))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|wosID              |author                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|WOS:000294118200001|[[[0000-0002-2231-7637, A-6943-2011, researcher_id, 1, Riet, Gerben ter, Gerben ter, Riet, Gerben ter, Riet]], [[0000-0002-0855-5313, J-2398-2014, researcher_id, 2, Schmid, Christopher H., Christopher H., Schmid, Christopher H., Schmid]], [[, B-3516-2013, researcher_id, 3, Song, Fujian, Fujian, Song, Fujian, Song]], [[0000-0003-0912-941X, A-4614-2008, researcher_id, 4, Singh, Sonal, Sonal, Singh, Sonal, Singh]], [[0000-0003-4721-1879, ABE-6298-2020, researcher_id, 5, Puhan, Milo, Milo, Puhan, Milo, Puhan]]]|\n",
      "+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wos_test = WoS.select(WoS.UID.alias(\"wosID\"), WoS.static_data.contributors.contributor.alias(\"author\"))\n",
    "\n",
    "wos_test.show(truncate=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WoS = WoS.filter(WoS.UID.contains('WOS:A1983QQ61700020'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8a292407eb45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwos_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWoS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWoS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUID\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wosID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWoS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontributors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"author\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwos_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/mag_spark/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mag_spark/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/.conda/envs/mag_spark/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mag_spark/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mag_spark/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wos_test = WoS.select(WoS.UID.alias(\"wosID\"), WoS.static_data.contributors.alias(\"author\"))\n",
    "\n",
    "wos_test.show(truncate=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o84.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1022 in stage 7.0 failed 4 times, most recent failure: Lost task 1022.3 in stage 7.0 (TID 2512, iuni5, executor 44): ExecutorLostFailure (executor 44 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits.  8.2 GB of 7 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead or disabling yarn.nodemanager.vmem-check-enabled because of YARN-4714.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1890)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2049)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2102)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2121)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4bea2cf4d933>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwos_test2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWoS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWoS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUID\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wosID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWoS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"author\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwos_test2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/mag_spark/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mag_spark/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mag_spark/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mag_spark/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o84.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1022 in stage 7.0 failed 4 times, most recent failure: Lost task 1022.3 in stage 7.0 (TID 2512, iuni5, executor 44): ExecutorLostFailure (executor 44 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits.  8.2 GB of 7 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead or disabling yarn.nodemanager.vmem-check-enabled because of YARN-4714.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1890)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2049)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2102)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2121)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "wos_test2 = WoS.select(WoS.UID.alias(\"wosID\"), WoS.static_data.summary.names.alias(\"author\"))\n",
    "\n",
    "wos_test2.show(truncate=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UID: string (nullable = true)\n",
      " |-- _r_id_disclaimer: string (nullable = true)\n",
      " |-- dynamic_data: struct (nullable = true)\n",
      " |    |-- cluster_related: struct (nullable = true)\n",
      " |    |    |-- identifiers: struct (nullable = true)\n",
      " |    |    |    |-- identifier: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |    |-- _type: string (nullable = true)\n",
      " |    |    |    |    |    |-- _value: string (nullable = true)\n",
      " |    |-- ic_related: struct (nullable = true)\n",
      " |    |    |-- oases: struct (nullable = true)\n",
      " |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |-- _is_OA: string (nullable = true)\n",
      " |    |    |    |-- oas: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |    |-- _type: string (nullable = true)\n",
      " |-- static_data: struct (nullable = true)\n",
      " |    |-- contributors: struct (nullable = true)\n",
      " |    |    |-- _count: long (nullable = true)\n",
      " |    |    |-- contributor: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- name: struct (nullable = true)\n",
      " |    |    |    |    |    |-- _orcid_id: string (nullable = true)\n",
      " |    |    |    |    |    |-- _r_id: string (nullable = true)\n",
      " |    |    |    |    |    |-- _role: string (nullable = true)\n",
      " |    |    |    |    |    |-- _seq_no: long (nullable = true)\n",
      " |    |    |    |    |    |-- display_name: string (nullable = true)\n",
      " |    |    |    |    |    |-- first_name: string (nullable = true)\n",
      " |    |    |    |    |    |-- full_name: string (nullable = true)\n",
      " |    |    |    |    |    |-- last_name: string (nullable = true)\n",
      " |    |-- fullrecord_metadata: struct (nullable = true)\n",
      " |    |    |-- abstracts: struct (nullable = true)\n",
      " |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |-- abstract: struct (nullable = true)\n",
      " |    |    |    |    |-- abstract_text: struct (nullable = true)\n",
      " |    |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |    |-- p: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- addresses: struct (nullable = true)\n",
      " |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |-- address_name: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- address_spec: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- _addr_no: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- city: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- country: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- full_address: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- organizations: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- organization: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- _pref: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- state: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- street: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- suborganizations: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- suborganization: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- zip: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- _location: string (nullable = true)\n",
      " |    |    |    |    |    |-- names: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- name: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |-- _addr_no: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- _orcid_id_tr: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- _r_id: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- _r_id_tr: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- _reprint: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- _role: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- _seq_no: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- display_name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- email_addr: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- first_name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- full_name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- last_name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- suffix: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- wos_standard: string (nullable = true)\n",
      " |    |    |-- category_info: struct (nullable = true)\n",
      " |    |    |    |-- headings: struct (nullable = true)\n",
      " |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |-- heading: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- subheadings: struct (nullable = true)\n",
      " |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |-- subheading: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- subjects: struct (nullable = true)\n",
      " |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |-- subject: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- _ascatype: string (nullable = true)\n",
      " |    |    |-- fund_ack: struct (nullable = true)\n",
      " |    |    |    |-- fund_text: struct (nullable = true)\n",
      " |    |    |    |    |-- p: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- grants: struct (nullable = true)\n",
      " |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |-- grant: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |-- _grant_source: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- grant_agency: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- _pref: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- grant_ids: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- grant_id: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- keywords: struct (nullable = true)\n",
      " |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |-- keyword: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- languages: struct (nullable = true)\n",
      " |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |-- language: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |    |-- _type: string (nullable = true)\n",
      " |    |    |-- normalized_doctypes: struct (nullable = true)\n",
      " |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |-- doctype: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- normalized_languages: struct (nullable = true)\n",
      " |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |-- language: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |    |-- _status: string (nullable = true)\n",
      " |    |    |    |    |    |-- _type: string (nullable = true)\n",
      " |    |    |-- references: struct (nullable = true)\n",
      " |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |-- reference: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- art_no: string (nullable = true)\n",
      " |    |    |    |    |    |-- assignee: string (nullable = true)\n",
      " |    |    |    |    |    |-- citedAuthor: string (nullable = true)\n",
      " |    |    |    |    |    |-- citedTitle: string (nullable = true)\n",
      " |    |    |    |    |    |-- citedWork: string (nullable = true)\n",
      " |    |    |    |    |    |-- doi: string (nullable = true)\n",
      " |    |    |    |    |    |-- page: string (nullable = true)\n",
      " |    |    |    |    |    |-- patent_no: string (nullable = true)\n",
      " |    |    |    |    |    |-- uid: string (nullable = true)\n",
      " |    |    |    |    |    |-- volume: string (nullable = true)\n",
      " |    |    |    |    |    |-- year: string (nullable = true)\n",
      " |    |    |-- reprint_addresses: struct (nullable = true)\n",
      " |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |-- address_name: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- address_spec: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- _addr_no: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- city: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- country: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- full_address: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- organizations: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- organization: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- _pref: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- state: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- street: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- suborganizations: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- suborganization: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- zip: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- _location: string (nullable = true)\n",
      " |    |    |    |    |    |-- names: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- name: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |-- _addr_no: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- _display: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- _reprint: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- _role: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- _seq_no: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- display_name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- email_addr: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- first_name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- full_name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- last_name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- suffix: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- wos_standard: string (nullable = true)\n",
      " |    |-- item: struct (nullable = true)\n",
      " |    |    |-- _xmlns:xsi: string (nullable = true)\n",
      " |    |    |-- _xsi:type: string (nullable = true)\n",
      " |    |    |-- bib_id: string (nullable = true)\n",
      " |    |    |-- bib_pagecount: struct (nullable = true)\n",
      " |    |    |    |-- _VALUE: long (nullable = true)\n",
      " |    |    |    |-- _type: string (nullable = true)\n",
      " |    |    |-- book_chapters: long (nullable = true)\n",
      " |    |    |-- book_desc: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- bk_binding: string (nullable = true)\n",
      " |    |    |    |    |-- bk_ordering: string (nullable = true)\n",
      " |    |    |    |    |-- bk_prepay: string (nullable = true)\n",
      " |    |    |    |    |-- bk_price: struct (nullable = true)\n",
      " |    |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |    |-- _amount: double (nullable = true)\n",
      " |    |    |    |    |    |-- _currency: string (nullable = true)\n",
      " |    |    |    |    |    |-- _desc: string (nullable = true)\n",
      " |    |    |    |    |    |-- _volumes: long (nullable = true)\n",
      " |    |    |    |    |-- bk_publisher: string (nullable = true)\n",
      " |    |    |-- book_notes: struct (nullable = true)\n",
      " |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |-- book_note: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- book_pages: long (nullable = true)\n",
      " |    |    |-- ids: struct (nullable = true)\n",
      " |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |-- _avail: string (nullable = true)\n",
      " |    |    |-- keywords_plus: struct (nullable = true)\n",
      " |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |-- keyword: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- reviewed_work: struct (nullable = true)\n",
      " |    |    |    |-- languages: struct (nullable = true)\n",
      " |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |-- language: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- rw_authors: struct (nullable = true)\n",
      " |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |-- rw_author: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- rw_year: double (nullable = true)\n",
      " |    |-- summary: struct (nullable = true)\n",
      " |    |    |-- EWUID: struct (nullable = true)\n",
      " |    |    |    |-- WUID: struct (nullable = true)\n",
      " |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |-- _coll_id: string (nullable = true)\n",
      " |    |    |    |-- edition: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |    |-- _value: string (nullable = true)\n",
      " |    |    |-- conferences: struct (nullable = true)\n",
      " |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |-- conference: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- _conf_id: long (nullable = true)\n",
      " |    |    |    |    |    |-- conf_dates: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- conf_date: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- _conf_end: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- _conf_start: string (nullable = true)\n",
      " |    |    |    |    |    |-- conf_infos: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- conf_info: string (nullable = true)\n",
      " |    |    |    |    |    |-- conf_locations: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- conf_location: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- conf_city: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- conf_host: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- conf_state: string (nullable = true)\n",
      " |    |    |    |    |    |-- conf_titles: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- conf_title: string (nullable = true)\n",
      " |    |    |    |    |    |-- sponsors: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- sponsor: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- doctypes: struct (nullable = true)\n",
      " |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |-- doctype: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- names: struct (nullable = true)\n",
      " |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |-- name: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- _addr_no: string (nullable = true)\n",
      " |    |    |    |    |    |-- _orcid_id_tr: string (nullable = true)\n",
      " |    |    |    |    |    |-- _r_id_tr: string (nullable = true)\n",
      " |    |    |    |    |    |-- _reprint: string (nullable = true)\n",
      " |    |    |    |    |    |-- _role: string (nullable = true)\n",
      " |    |    |    |    |    |-- _seq_no: long (nullable = true)\n",
      " |    |    |    |    |    |-- display_name: string (nullable = true)\n",
      " |    |    |    |    |    |-- email_addr: string (nullable = true)\n",
      " |    |    |    |    |    |-- first_name: string (nullable = true)\n",
      " |    |    |    |    |    |-- full_name: string (nullable = true)\n",
      " |    |    |    |    |    |-- last_name: string (nullable = true)\n",
      " |    |    |    |    |    |-- suffix: string (nullable = true)\n",
      " |    |    |    |    |    |-- wos_standard: string (nullable = true)\n",
      " |    |    |-- pub_info: struct (nullable = true)\n",
      " |    |    |    |-- _coverdate: string (nullable = true)\n",
      " |    |    |    |-- _early_access_date: string (nullable = true)\n",
      " |    |    |    |-- _early_access_month: long (nullable = true)\n",
      " |    |    |    |-- _early_access_year: long (nullable = true)\n",
      " |    |    |    |-- _has_abstract: string (nullable = true)\n",
      " |    |    |    |-- _issue: string (nullable = true)\n",
      " |    |    |    |-- _part_no: string (nullable = true)\n",
      " |    |    |    |-- _pubmonth: string (nullable = true)\n",
      " |    |    |    |-- _pubtype: string (nullable = true)\n",
      " |    |    |    |-- _pubyear: long (nullable = true)\n",
      " |    |    |    |-- _sortdate: date (nullable = true)\n",
      " |    |    |    |-- _special_issue: string (nullable = true)\n",
      " |    |    |    |-- _supplement: string (nullable = true)\n",
      " |    |    |    |-- _vol: string (nullable = true)\n",
      " |    |    |    |-- page: struct (nullable = true)\n",
      " |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |-- _begin: string (nullable = true)\n",
      " |    |    |    |    |-- _end: string (nullable = true)\n",
      " |    |    |    |    |-- _page_count: long (nullable = true)\n",
      " |    |    |-- publishers: struct (nullable = true)\n",
      " |    |    |    |-- publisher: struct (nullable = true)\n",
      " |    |    |    |    |-- address_spec: struct (nullable = true)\n",
      " |    |    |    |    |    |-- _addr_no: long (nullable = true)\n",
      " |    |    |    |    |    |-- city: string (nullable = true)\n",
      " |    |    |    |    |    |-- full_address: string (nullable = true)\n",
      " |    |    |    |    |-- names: struct (nullable = true)\n",
      " |    |    |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |    |    |-- name: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- _addr_no: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- _role: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- _seq_no: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- display_name: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- full_name: string (nullable = true)\n",
      " |    |    |-- titles: struct (nullable = true)\n",
      " |    |    |    |-- _count: long (nullable = true)\n",
      " |    |    |    |-- title: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |    |-- _translated: string (nullable = true)\n",
      " |    |    |    |    |    |-- _type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "WoS.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o122.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 4 times, most recent failure: Lost task 0.3 in stage 8.0 (TID 2831, iuni9, executor 22): ExecutorLostFailure (executor 22 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits.  19.6 GB of 18 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead or disabling yarn.nodemanager.vmem-check-enabled because of YARN-4714.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1890)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2049)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 32 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mag_spark/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mag_spark/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mag_spark/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mag_spark/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o122.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 4 times, most recent failure: Lost task 0.3 in stage 8.0 (TID 2831, iuni9, executor 22): ExecutorLostFailure (executor 22 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits.  19.6 GB of 18 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead or disabling yarn.nodemanager.vmem-check-enabled because of YARN-4714.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1890)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2049)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 32 more\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "WoS.coalesce(16).write.format('json').save('/N/slate/maahutch/wos20_check/oneRec.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "wosOutput1.coalesce(16).write.option(\"header\",\"True\") \\\n",
    "                               .option(\"sep\",\"\\t\") \\\n",
    "                               .option(\"quoteAll\", True) \\\n",
    "                               .mode(\"overwrite\") \\\n",
    "                               .csv('/WoSraw_2020_all/nodes_test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
